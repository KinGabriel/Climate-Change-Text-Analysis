{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:08:25.082882Z",
     "start_time": "2025-06-28T11:08:25.069355Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NJ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import warnings\n",
    "import unicodedata\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "TOKENIZERS_PARALLELISM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b569def11035c6f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:09:06.503708Z",
     "start_time": "2025-06-28T11:08:25.089171Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../the-reddit-climate-change-dataset-comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc5667767300dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:10:00.982811Z",
     "start_time": "2025-06-28T11:09:06.520960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved balanced dataset with 50000 records to ../reduced_dataset.csv\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "negative    16668\n",
      "positive    16666\n",
      "neutral     16666\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def reduce_dataset_balanced(input_file, output_file, target_size=50000):\n",
    "    \"\"\"\n",
    "    Reduce a dataset to a target size while maintaining balanced sentiment distribution.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to input CSV file\n",
    "        output_file (str): Path to save reduced CSV file\n",
    "        target_size (int): Desired number of records in output (default: 50000)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Categorize sentiment\n",
    "    df['sentiment_category'] = np.where(\n",
    "        df['sentiment'] < 0, 'negative',\n",
    "        np.where(df['sentiment'] == 0, 'neutral', 'positive')\n",
    "    )\n",
    "\n",
    "    # Calculate target size for each category (equal distribution)\n",
    "    category_size = target_size // 3\n",
    "\n",
    "    # Sample from each category\n",
    "    samples = []\n",
    "    for category in ['negative', 'neutral', 'positive']:\n",
    "        category_df = df[df['sentiment_category'] == category]\n",
    "\n",
    "        # If category has fewer samples than needed, take all\n",
    "        n_samples = min(category_size, len(category_df))\n",
    "\n",
    "        # Random sample without replacement\n",
    "        sample = category_df.sample(n=n_samples, random_state=42)\n",
    "        samples.append(sample)\n",
    "\n",
    "    # Combine samples\n",
    "    reduced_df = pd.concat(samples)\n",
    "\n",
    "    # If total is less than target due to rounding, sample more from largest category\n",
    "    if len(reduced_df) < target_size:\n",
    "        remaining = target_size - len(reduced_df)\n",
    "        # Find largest category\n",
    "        counts = reduced_df['sentiment_category'].value_counts()\n",
    "        largest_category = counts.idxmax()\n",
    "        # Get additional samples from largest category\n",
    "        category_df = df[df['sentiment_category'] == largest_category]\n",
    "        # Exclude already sampled rows\n",
    "        category_df = category_df[~category_df.index.isin(reduced_df.index)]\n",
    "        additional_samples = category_df.sample(n=remaining, random_state=42)\n",
    "        reduced_df = pd.concat([reduced_df, additional_samples])\n",
    "\n",
    "    # Remove temporary column and shuffle\n",
    "    reduced_df = reduced_df.drop(columns=['sentiment_category'])\n",
    "    reduced_df = reduced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    reduced_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved balanced dataset with {len(reduced_df)} records to {output_file}\")\n",
    "    print(\"Sentiment distribution:\")\n",
    "    print(reduced_df['sentiment'].apply(\n",
    "        lambda x: 'negative' if x < 0 else 'neutral' if x == 0 else 'positive'\n",
    "    ).value_counts())\n",
    "\n",
    "reduce_dataset_balanced('../the-reddit-climate-change-dataset-comments.csv', '../reduced_dataset.csv', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30df5f3a3d61da2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:10:04.857959Z",
     "start_time": "2025-06-28T11:10:01.015418Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../reduced_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4da08e539472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:10:05.011545Z",
     "start_time": "2025-06-28T11:10:04.874965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in each column:\n",
      "type                0\n",
      "id                  0\n",
      "subreddit.id        0\n",
      "subreddit.name      0\n",
      "subreddit.nsfw      0\n",
      "created_utc         0\n",
      "permalink           0\n",
      "body                0\n",
      "sentiment         451\n",
      "score               0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    " #1. Check for null values\n",
    "print(\"Null values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 2. Check for duplicate rows\n",
    "print(\"\\nNumber of duplicate rows:\")\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "# 3. Drop the unnecessary columns\n",
    "df = df.drop(columns=['type', 'id', 'subreddit.id', 'subreddit.name','subreddit.nsfw', 'created_utc', 'permalink', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b708963c2d5b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:10:05.151271Z",
     "start_time": "2025-06-28T11:10:05.029513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of comments containing links:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_accented_chars(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+|www\\.\\S+|https\\S+', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def remove_symbols(text):\n",
    "    return re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "\n",
    "df['body'] = df['body'].apply(remove_accented_chars)\n",
    "df['body'] = df['body'].apply(remove_links)\n",
    "df['body'] = df['body'].apply(remove_symbols)\n",
    "\n",
    "#to check if links were removed\n",
    "print(\"\\nNumber of comments containing links:\")\n",
    "(df['body'].str.contains(\"http\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f5453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].str.replace(r'http\\S+|www\\.\\S+|https\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b5acee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with symbols in 'body': 0\n"
     ]
    }
   ],
   "source": [
    "#to check if symbols were removed\n",
    "symbol_rows = df[df['body'].str.contains(r'[^A-Za-z\\s]', regex=True)]\n",
    "print(f\"Number of rows with symbols in 'body': {len(symbol_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6cd759f6a4ed329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:10:21.733460Z",
     "start_time": "2025-06-28T11:10:05.168848Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People need to do this kind of thing more ofte...</td>\n",
       "      <td>0.8610</td>\n",
       "      <td>[People, need, to, do, this, kind, of, thing, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thats cute if things dont reverse course in ou...</td>\n",
       "      <td>-0.1759</td>\n",
       "      <td>[Thats, cute, if, things, dont, reverse, cours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whats interesting is that you are arguing with...</td>\n",
       "      <td>-0.0896</td>\n",
       "      <td>[Whats, interesting, is, that, you, are, argui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i can agree with that a lot of the media is ow...</td>\n",
       "      <td>-0.8176</td>\n",
       "      <td>[i, can, agree, with, that, a, lot, of, the, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The rising seas are due to global warming but ...</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>[The, rising, seas, are, due, to, global, warm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  sentiment  \\\n",
       "0  People need to do this kind of thing more ofte...     0.8610   \n",
       "1  Thats cute if things dont reverse course in ou...    -0.1759   \n",
       "2  Whats interesting is that you are arguing with...    -0.0896   \n",
       "3  i can agree with that a lot of the media is ow...    -0.8176   \n",
       "4  The rising seas are due to global warming but ...     0.0772   \n",
       "\n",
       "                                      tokenized_body  \n",
       "0  [People, need, to, do, this, kind, of, thing, ...  \n",
       "1  [Thats, cute, if, things, dont, reverse, cours...  \n",
       "2  [Whats, interesting, is, that, you, are, argui...  \n",
       "3  [i, can, agree, with, that, a, lot, of, the, m...  \n",
       "4  [The, rising, seas, are, due, to, global, warm...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "df['tokenized_body'] = df['body'].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c551c510214d24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:12:16.150324Z",
     "start_time": "2025-06-28T11:10:21.796462Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_body</th>\n",
       "      <th>lemmatized_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People need to do this kind of thing more ofte...</td>\n",
       "      <td>0.8610</td>\n",
       "      <td>[People, need, to, do, this, kind, of, thing, ...</td>\n",
       "      <td>[People, need, to, do, this, kind, of, thing, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thats cute if things dont reverse course in ou...</td>\n",
       "      <td>-0.1759</td>\n",
       "      <td>[Thats, cute, if, things, dont, reverse, cours...</td>\n",
       "      <td>[Thats, cute, if, thing, dont, reverse, course...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whats interesting is that you are arguing with...</td>\n",
       "      <td>-0.0896</td>\n",
       "      <td>[Whats, interesting, is, that, you, are, argui...</td>\n",
       "      <td>[Whats, interest, be, that, you, be, argue, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i can agree with that a lot of the media is ow...</td>\n",
       "      <td>-0.8176</td>\n",
       "      <td>[i, can, agree, with, that, a, lot, of, the, m...</td>\n",
       "      <td>[i, can, agree, with, that, a, lot, of, the, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The rising seas are due to global warming but ...</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>[The, rising, seas, are, due, to, global, warm...</td>\n",
       "      <td>[The, rise, sea, be, due, to, global, warm, bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  sentiment  \\\n",
       "0  People need to do this kind of thing more ofte...     0.8610   \n",
       "1  Thats cute if things dont reverse course in ou...    -0.1759   \n",
       "2  Whats interesting is that you are arguing with...    -0.0896   \n",
       "3  i can agree with that a lot of the media is ow...    -0.8176   \n",
       "4  The rising seas are due to global warming but ...     0.0772   \n",
       "\n",
       "                                      tokenized_body  \\\n",
       "0  [People, need, to, do, this, kind, of, thing, ...   \n",
       "1  [Thats, cute, if, things, dont, reverse, cours...   \n",
       "2  [Whats, interesting, is, that, you, are, argui...   \n",
       "3  [i, can, agree, with, that, a, lot, of, the, m...   \n",
       "4  [The, rising, seas, are, due, to, global, warm...   \n",
       "\n",
       "                                     lemmatized_body  \n",
       "0  [People, need, to, do, this, kind, of, thing, ...  \n",
       "1  [Thats, cute, if, thing, dont, reverse, course...  \n",
       "2  [Whats, interest, be, that, you, be, argue, wi...  \n",
       "3  [i, can, agree, with, that, a, lot, of, the, m...  \n",
       "4  [The, rise, sea, be, due, to, global, warm, bu...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Map POS tag\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "\n",
    "# Apply lemmatization directly (tokenized_body is already a list)\n",
    "df['lemmatized_body'] = df['tokenized_body'].apply(lemmatize_tokens)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53db2dae0900f158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:12:16.259613Z",
     "start_time": "2025-06-28T11:12:16.245077Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['tokenized_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f5d8a60dc6009e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:12:37.711084Z",
     "start_time": "2025-06-28T11:12:16.300888Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\NJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tokenized_body'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NJ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'tokenized_body'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlemmatize_tokens\u001b[39m(tokens):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [lemmatizer.lemmatize(token, get_wordnet_pos(token)) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlemmatized_body\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtokenized_body\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(lemmatize_tokens)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NJ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NJ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'tokenized_body'"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "\n",
    "df['lemmatized_body'] = df['tokenized_body'].apply(lemmatize_tokens)\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "CONTRACTIONS = {\n",
    "    r\"won't\": \"will not\", r\"can't\": \"can not\", r\"n't\": \" not\",\n",
    "    r\"'re\": \" are\", r\"'s\": \" is\", r\"'d\": \" would\",\n",
    "    r\"'ll\": \" will\", r\"'ve\": \" have\", r\"'m\": \" am\"\n",
    "}\n",
    "\n",
    "def expand_contractions(token):\n",
    "    for contraction, expansion in CONTRACTIONS.items():\n",
    "        token = re.sub(contraction, expansion, token)\n",
    "    return token\n",
    "\n",
    "def clean_and_remove_stopwords(token_list):\n",
    "    cleaned_tokens = []\n",
    "    for token in token_list:\n",
    "        if not re.fullmatch(r\"[a-zA-Z'-]+\", str(token)):\n",
    "            continue\n",
    "        token = expand_contractions(token)\n",
    "        subtokens = token.split()\n",
    "        for subtoken in subtokens:\n",
    "            subtoken = re.sub(r\"'s$\", \"\", subtoken)\n",
    "            if not subtoken:\n",
    "                continue\n",
    "            if subtoken.lower() not in STOPWORDS:\n",
    "                cleaned_tokens.append(subtoken)\n",
    "    return cleaned_tokens\n",
    "\n",
    "df['lemmatized_body'] = df['lemmatized_body'].apply(clean_and_remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae4415",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385824706b57fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:12:37.741366Z",
     "start_time": "2025-06-28T11:12:37.718355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lemmatized_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People need to do this kind of thing more ofte...</td>\n",
       "      <td>0.8610</td>\n",
       "      <td>[People, need, kind, thing, often, obviously, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thats cute if things dont reverse course in ou...</td>\n",
       "      <td>-0.1759</td>\n",
       "      <td>[Thats, cute, thing, dont, reverse, course, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whats interesting is that you are arguing with...</td>\n",
       "      <td>-0.0896</td>\n",
       "      <td>[Whats, interest, argue, statement, almost, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i can agree with that a lot of the media is ow...</td>\n",
       "      <td>-0.8176</td>\n",
       "      <td>[agree, lot, medium, large, part, billionaire,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The rising seas are due to global warming but ...</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>[rise, sea, due, global, warm, flood, probably...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  sentiment  \\\n",
       "0  People need to do this kind of thing more ofte...     0.8610   \n",
       "1  Thats cute if things dont reverse course in ou...    -0.1759   \n",
       "2  Whats interesting is that you are arguing with...    -0.0896   \n",
       "3  i can agree with that a lot of the media is ow...    -0.8176   \n",
       "4  The rising seas are due to global warming but ...     0.0772   \n",
       "\n",
       "                                     lemmatized_body  \n",
       "0  [People, need, kind, thing, often, obviously, ...  \n",
       "1  [Thats, cute, thing, dont, reverse, course, wo...  \n",
       "2  [Whats, interest, argue, statement, almost, sc...  \n",
       "3  [agree, lot, medium, large, part, billionaire,...  \n",
       "4  [rise, sea, due, global, warm, flood, probably...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1da199d9691a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:12:38.789864Z",
     "start_time": "2025-06-28T11:12:37.789433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert string representation of list to actual list if needed\n",
    "if isinstance(df['lemmatized_body'].iloc[0], str):\n",
    "    df['lemmatized_body'] = df['lemmatized_body'].apply(ast.literal_eval)\n",
    "\n",
    "# Convert all words in lemmatized_body lists to lowercase\n",
    "df['lemmatized_body'] = df['lemmatized_body'].apply(lambda lst: [word.lower() for word in lst])\n",
    "\n",
    "# Rename lemmatized_body to preprocessed_body\n",
    "df = df.rename(columns={'lemmatized_body': 'preprocessed_body'})\n",
    "\n",
    "# Reorder columns to put body first\n",
    "df = df[['body', 'preprocessed_body', 'sentiment']]\n",
    "\n",
    "# Save the processed DataFrame back to CSV to be used for training\n",
    "df.to_csv('../preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d0306458c4769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T11:12:39.754326Z",
     "start_time": "2025-06-28T11:12:39.740168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>preprocessed_body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People need to do this kind of thing more ofte...</td>\n",
       "      <td>[people, need, kind, thing, often, obviously, ...</td>\n",
       "      <td>0.8610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thats cute if things dont reverse course in ou...</td>\n",
       "      <td>[thats, cute, thing, dont, reverse, course, wo...</td>\n",
       "      <td>-0.1759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whats interesting is that you are arguing with...</td>\n",
       "      <td>[whats, interest, argue, statement, almost, sc...</td>\n",
       "      <td>-0.0896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i can agree with that a lot of the media is ow...</td>\n",
       "      <td>[agree, lot, medium, large, part, billionaire,...</td>\n",
       "      <td>-0.8176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The rising seas are due to global warming but ...</td>\n",
       "      <td>[rise, sea, due, global, warm, flood, probably...</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  People need to do this kind of thing more ofte...   \n",
       "1  Thats cute if things dont reverse course in ou...   \n",
       "2  Whats interesting is that you are arguing with...   \n",
       "3  i can agree with that a lot of the media is ow...   \n",
       "4  The rising seas are due to global warming but ...   \n",
       "\n",
       "                                   preprocessed_body  sentiment  \n",
       "0  [people, need, kind, thing, often, obviously, ...     0.8610  \n",
       "1  [thats, cute, thing, dont, reverse, course, wo...    -0.1759  \n",
       "2  [whats, interest, argue, statement, almost, sc...    -0.0896  \n",
       "3  [agree, lot, medium, large, part, billionaire,...    -0.8176  \n",
       "4  [rise, sea, due, global, warm, flood, probably...     0.0772  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
