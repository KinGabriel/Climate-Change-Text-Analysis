{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3a6f63-ffbd-471e-ac79-d1a8fd5ab697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Set Performance\n",
      "========================\n",
      "Accuracy : 0.9989\n",
      "Precision: 0.9989\n",
      "Recall   : 0.9989\n",
      "F1 Score : 0.9989\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4585    5]\n",
      " [   5 4405]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00      4590\n",
      "    Positive       1.00      1.00      1.00      4410\n",
      "\n",
      "    accuracy                           1.00      9000\n",
      "   macro avg       1.00      1.00      1.00      9000\n",
      "weighted avg       1.00      1.00      1.00      9000\n",
      "\n",
      "\n",
      " Unseen Validation Set Performance\n",
      "=====================================\n",
      "Accuracy : 0.9976\n",
      "Precision: 0.9967\n",
      "Recall   : 0.9984\n",
      "F1 Score : 0.9976\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2542    8]\n",
      " [   4 2446]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00      2550\n",
      "    Positive       1.00      1.00      1.00      2450\n",
      "\n",
      "    accuracy                           1.00      5000\n",
      "   macro avg       1.00      1.00      1.00      5000\n",
      "weighted avg       1.00      1.00      1.00      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from gensim import corpora, models\n",
    "from gensim.matutils import sparse2full\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "# --- Load Preprocessed Data ---\n",
    "df = pd.read_csv(\"preprocessed.csv\")\n",
    "\n",
    "# --- Tokenize the 'preprocessed_body' Column ---\n",
    "df['tokens'] = df['preprocessed_body'].apply(literal_eval)\n",
    "\n",
    "# --- Create Bag of Words (BoW) ---\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=2000)\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# --- Train LDA Model with 3 Topics ---\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=3,\n",
    "    random_state=42,\n",
    "    passes=10\n",
    ")\n",
    "\n",
    "# --- LDA Vectors (Topic Distribution per Document) ---\n",
    "lda_vectors = []\n",
    "for bow in corpus:\n",
    "    doc_topics = lda_model.get_document_topics(bow)\n",
    "    dense_vector = sparse2full(doc_topics, lda_model.num_topics)\n",
    "    lda_vectors.append(dense_vector)\n",
    "\n",
    "# --- BoW Vectors (Dense Format) ---\n",
    "bow_vectors = [sparse2full(doc_bow, len(dictionary)) for doc_bow in corpus]\n",
    "\n",
    "# --- Sentiment Scores ---\n",
    "df['sentiment'] = df['preprocessed_body'].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "# --- Combine Features: [BoW + LDA + Sentiment] ---\n",
    "combined_features = []\n",
    "for bow_vec, lda_vec, sentiment in zip(bow_vectors, lda_vectors, df['sentiment']):\n",
    "    feature_vector = np.concatenate([bow_vec, lda_vec, [sentiment]])\n",
    "    combined_features.append(feature_vector)\n",
    "\n",
    "X = np.array(combined_features)\n",
    "\n",
    "# --- Generate Binary Labels from Sentiment ---\n",
    "df['label'] = df['sentiment'].apply(lambda s: 1 if s > 0 else 0)\n",
    "y = df['label'].values\n",
    "\n",
    "# Step 1: Split 10% for Unseen Validation Set ---\n",
    "X_remaining, X_unseen, y_remaining, y_unseen, df_remaining, df_unseen = train_test_split(\n",
    "    X, y, df, test_size=0.10, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split Remaining 90% into 80% Train, 20% Test ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_remaining, y_remaining, test_size=0.20, random_state=42, stratify=y_remaining\n",
    ")\n",
    "\n",
    "# --- Train Classifier ---\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- Predict on Test Set ---\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# --- Function to Display Performance Summary ---\n",
    "def print_performance_report(title, y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n {title}\")\n",
    "    print(\"=\" * (len(title) + 4))\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive'], zero_division=0))\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "print_performance_report(\"Test Set Performance\", y_test, y_pred)\n",
    "\n",
    "# --- Evaluate on Unseen Validation Set ---\n",
    "print_performance_report(\"Unseen Validation Set Performance\", y_unseen, y_unseen_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f889f2-fa8c-46c0-87b4-66c3bcf2eeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8063e-0aa2-4027-9f34-b761f23a6b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
